# 决策树和随机森林

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012223373.jpeg" alt="【机器学习】决策树（上）——ID3、C4.5、CART（非常详细）" style="zoom: 33%;" /> <img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012223466.jpeg" alt="【机器学习】决策树（中）——Random Forest、Adaboost、GBDT （非常详细）" style="zoom: 33%;" /><img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012223447.png" alt="【机器学习】决策树（下）——XGBoost、LightGBM（非常详细）" style="zoom: 19%;" />

[【机器学习】决策树（上）——ID3、C4.5、CART（非常详细） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/85731206)

[【机器学习】决策树（中）——Random Forest、Adaboost、GBDT （非常详细） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/86263786)

[【机器学习】决策树（下）——XGBoost、LightGBM（非常详细） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/87885678)



## 决策树
以分类任务为代表的决策树模型，是一种对样本特征构建不同分支的树形结构。
决策树由节点和有向边组成，其中节点包括内部节点（圆）和叶节点（方框）。内部节点表示一个特征或属性，叶节点表示一个具体类别。

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012129713.png" alt="image-20230901212924580" style="zoom:33%;" />

预测时，从最顶端的根节点开始向下搜索，直到某一个叶子节点结束。下图的红线代表了一条搜索路线，决策树最终输出类别C。

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012129858.png" alt="image-20230901212943810" style="zoom:33%;" />

### 决策树的特征选择

决策树的特征选择一般有3种量化方法：**信息增益、信息增益率、基尼指数**。

#### 信息增益（ID3）

**熵越大，随机性越大，结果越不确定**。

现在我们有一份数据集D（例如贷款信息登记表）和特征A（例如年龄），则**A的信息增益就是D本身的熵与特征A给定条件下D的条件熵之差**，即：

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012136406.png" alt="image-20230901213625372" style="zoom: 50%;" align="left" />



<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012130248.png" alt="image-20230901213046191" style="zoom: 67%;" />

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012131061.png" alt="image-20230901213124022" style="zoom:67%;" />

##### 思想

从信息论的知识中我们知道：信息熵越大，从而样本纯度越低，。ID3 算法的核心思想就是以信息增益来度量特征选择，选择信息增益最大的特征进行分裂。算法采用自顶向下的贪婪搜索遍历可能的决策树空间（C4.5 也是贪婪搜索）。 其大致步骤为：

1. 初始化特征集合和数据集合；
2. 计算数据集合信息熵和所有特征的条件熵，选择信息增益最大的特征作为当前决策节点；
3. 更新数据集合和特征集合（删除上一步使用的特征，并按照特征值来划分不同分支的数据集合）；
4. 重复 2，3 两步，若子集值包含单一特征，则为分支叶子节点。

##### 缺点

- ID3 没有剪枝策略，容易过拟合；
- 信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1；
- 只能用于处理离散分布的特征；
- 没有考虑缺失值。

#### **信息增益率** C4.5

==当某个特征具有多种候选值时，信息增益容易偏大，造成误差。引入信息增益率可以校正这一问题。==

==（C4.5 算法最大的特点是克服了 ID3 对特征数目的偏重这一缺点，引入信息增益率来作为分类标准。）==

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012138365.png" alt="image-20230901213808330" style="zoom:67%;" align="left" />

##### 思想

C4.5 相对于 ID3 的缺点对应有以下改进方式：

- 引入悲观剪枝策略进行后剪枝；
- 引入信息增益率作为划分标准；
- 将连续特征离散化，假设 n 个样本的连续特征 A 有 m 个取值，C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点；
- 对于缺失值的处理可以分为两个子问题：
- 问题一：在特征值缺失的情况下进行划分特征的选择？（即如何计算特征的信息增益率）
- 问题二：选定该划分特征，对于缺失该特征值的样本如何处理？（即到底把这个样本划分到哪个结点里）
- 针对问题一，C4.5 的做法是：对于具有缺失值特征，用没有缺失的样本子集所占比重来折算；
- 针对问题二，C4.5 的做法是：将样本同时划分到所有子节点，不过要调整样本的权重值，其实也就是以不同概率划分到不同节点中。

##### 剪枝策略

###### **预剪枝**

在节点划分前来确定是否继续增长，及早停止增长的主要方法有：

- 节点内数据样本低于某一阈值；
- 所有节点特征都已分裂；
- 节点划分前准确率比划分后准确率高。

预剪枝不仅可以降低过拟合的风险而且还可以减少训练时间，但另一方面它是基于“贪心”策略，会带来欠拟合风险。



###### 后剪枝

在已经生成的决策树上进行剪枝，从而得到简化版的剪枝决策树。

C4.5 采用的**悲观剪枝方法**，用递归的方式从低往上针对每一个非叶子节点，评估用一个最佳叶子节点去代替这课子树是否有益。如果剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉。C4.5 通过训练数据集上的错误分类数量来估算未知样本上的错误率。

后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但同时其训练时间会大的多。



##### 缺点

- 剪枝策略可以再优化；
- C4.5 用的是多叉树，用二叉树效率更高；
- C4.5 只能用于分类；
- C4.5 使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算；
- C4.5 在构造树的过程中，对数值属性值需要按照其大小进行排序，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。



#### **基尼指数 **CART算法

**优先选择基尼指数最小的特征，由此生成决策树，称为CART算法**。

ID3 和 C4.5 虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但是其生成的决策树分支、规模都比较大，CART 算法的二分法可以简化决策树的规模，提高生成决策树的效率。

##### 思想

CART 包含的基本过程有分裂，剪枝和树选择。

- **分裂：**分裂过程是一个二叉递归划分过程，其输入和预测特征既可以是连续型的也可以是离散型的，CART 没有停止准则，会一直生长下去；
- **剪枝：**采用**代价复杂度剪枝**，从最大树开始，每次选择训练数据熵对整体性能贡献最小的那个分裂节点作为下一个剪枝对象，直到只剩下根节点。CART 会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树；
- **树选择：**用单独的测试集评估每棵剪枝树的预测性能（也可以用交叉验证）。

CART 在 C4.5 的基础上进行了很多提升。

- C4.5 为多叉树，运算速度慢，CART 为二叉树，运算速度快；
- C4.5 只能分类，CART 既可以分类也可以回归；
- CART 使用 Gini 系数作为变量的不纯度量，减少了大量的对数运算；
- CART 采用代理测试来估计缺失值，而 C4.5 以不同概率划分到不同节点中；
- CART 采用“基于代价复杂度剪枝”方法进行剪枝，而 C4.5 采用悲观剪枝方法。

##### 划分标准

熵模型拥有大量耗时的对数运算，基尼指数在简化模型的同时还保留了熵模型的优点。基尼指数代表了模型的不纯度，基尼系数越小，不纯度越低，特征越好。这和信息增益（率）正好相反。

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012209685.png" alt="image-20230901220918643" style="zoom:50%;" />

其中 k 代表类别。

基尼指数反映了从**数据集中随机抽取两个样本，其类别标记不一致的概率**。因此基尼指数越小，则数据集纯度越高。基尼指数偏向于特征值较多的特征，类似信息增益。基尼指数可以用来度量任何不均匀分布，是介于 0~1 之间的数，0 是完全相等，1 是完全不相等，

此外，当 CART 为二分类，其表达式为：

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012210817.png" alt="image-20230901221045785" style="zoom:50%;" />

我们可以看到在平方运算和二分类的情况下，其运算更加简单。当然其性能也与熵模型非常接近。

那么问题来了：基尼指数与熵模型性能接近，但到底与熵模型的差距有多大呢？

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012211379.png" alt="image-20230901221129308" style="zoom:50%;" />

#####  缺失值处理

上文说到，模型对于缺失值的处理会分为两个子问题：

1. 如何在特征值缺失的情况下进行划分特征的选择？
2. 选定该划分特征，模型对于缺失该特征值的样本该进行怎样处理？

对于问题 1，CART 一开始严格要求分裂特征评估时只能使用在该特征上没有缺失值的那部分数据，在后续版本中，CART 算法使用了一种惩罚机制来抑制提升值，从而反映出缺失值的影响（例如，如果一个特征在节点的 20% 的记录是缺失的，那么这个特征就会减少 20% 或者其他数值）。

对于问题 2，CART 算法的机制是为树的每个节点都找到代理分裂器，无论在训练数据上得到的树是否有缺失值都会这样做。在代理分裂器中，特征的分值必须超过默认规则的性能才有资格作为代理（即代理就是代替缺失值特征作为划分特征的特征），当 CART 树中遇到缺失值时，这个实例划分到左边还是右边是决定于其排名最高的代理，如果这个代理的值也缺失了，那么就使用排名第二的代理，以此类推，如果所有代理值都缺失，那么默认规则就是把样本划分到较大的那个子节点。代理分裂器可以确保无缺失训练数据上得到的树可以用来处理包含确实值的新数据。



##### 剪枝策略



##### 类别不平衡

![image-20230901221539220](https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012215288.png)



#### 总结

最后通过总结的方式对比下 ID3、C4.5 和 CART 三者之间的差异。

除了之前列出来的划分标准、剪枝策略、连续值确实值处理方式等之外，我再介绍一些其他差异：

- **划分标准的差异：**ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。
- **使用场景的差异：**ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；
- **样本数据的差异：**ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；
- **样本特征的差异：**ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征；
- **剪枝策略的差异：**ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。



### 决策树剪枝

决策树生成算法递归产生一棵决策树，直到结束划分。什么时候结束呢？

- 样本属于同一种类型
- 没有特征可以分割

这样得到的决策树往往对训练数据分类非常精准，但是对于未知数据表现比较差。

原因在于基于训练集构造的决策树过于复杂，产生过拟合。所以需要对决策树简化，砍掉多余的分支，提高泛化能力。

决策树剪枝一般有两种方法：

- **预剪枝**：在树的生成过程中剪枝。**基于贪心策略，可能造成局部最优**
- **后剪枝**：等树全部生成后剪枝。**运算量较大，但是比较精准**

决策树剪枝往往通过**极小化决策树整体的损失函数实现**。

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012151160.webp" alt="img" style="zoom: 67%;" />

![image-20230901215206687](https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012152732.png)







### CART算法

**CART（Classification and Regression Tree，分类回归树），从名字就可以看出其不仅可以用于分类，也可以应用于回归。其回归树的建立算法上与分类树部分相似，这里简单介绍下不同之处。**

CART表示分类回归决策树，同样由特征选择、树的生成及剪枝组成，可以处理分类和回归任务。

相比之下，**ID3和C4.5算法只能处理分类任务**。

CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，依次递归地二分每个特征。

**CART 对回归树采用平方误差最小化准则，对分类树用基尼指数最小化准则**。

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012216879.png" alt="image-20230901221659808" style="zoom:67%;" />

## 集成学习

常见的集成学习框架有三种：Bagging，Boosting 和 Stacking。三种集成学习框架在基学习器的产生和综合结果的方式上会有些区别，我们先做些简单的介绍。

###  Bagging

Bagging 全称叫 **B**ootstrap **agg**regat**ing**，每个基学习器都会对训练集进行有放回抽样得到子训练集，比较著名的采样法为 0.632 自助法。每个基学习器基于不同子训练集进行训练，并综合所有基学习器的预测值得到最终的预测结果。Bagging 常用的综合方法是投票法，票数最多的类别为预测类别。

![img](https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012253296.webp)

### Boosting

Boosting 训练过程为阶梯状，基模型的训练是有顺序的，每个基模型都会在前一个基模型学习的基础上进行学习，最终综合所有基模型的预测值产生最终的预测结果，用的比较多的综合方式为加权法。

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012253923.webp" alt="img" style="zoom:80%;" />

### Stacking

Stacking 是先用全部数据训练好基模型，然后每个基模型都对每个训练样本进行的预测，其预测值将作为训练样本的特征值，最终会得到新的训练样本，然后基于新的训练样本进行训练得到模型，然后得到最终预测结果。

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012254222.webp" alt="img" style="zoom:80%;" />

那么，为什么集成学习会好于单个学习器呢？原因可能有三：

1. 训练样本可能无法选择出最好的单个学习器，由于没法选择出最好的学习器，所以干脆结合起来一起用；
2. 假设能找到最好的学习器，但由于算法运算的限制无法找到最优解，只能找到次优解，采用集成学习可以弥补算法的不足；
3. 可能算法无法得到最优解，而集成学习能够得到近似解。比如说最优解是一条对角线，而单个决策树得到的结果只能是平行于坐标轴的，但是集成学习可以去拟合这条对角线。

## 偏差与方差

### 集成学习的偏差与方差

偏差（Bias）描述的是预测值和真实值之差；方差（Variance）描述的是预测值作为随机变量的离散程度。放一场很经典的图：

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012256046.webp" alt="img" style="zoom:67%;" />

模型的偏差与方差

- **偏差：**描述样本拟合出的模型的预测结果的期望与样本真实结果的差距，要想偏差表现的好，就需要复杂化模型，增加模型的参数，但这样容易过拟合，过拟合对应上图的 High Variance，点会很分散。低偏差对应的点都打在靶心附近，所以喵的很准，但不一定很稳；
- **方差：**描述样本上训练出来的模型在测试集上的表现，要想方差表现的好，需要简化模型，减少模型的复杂度，但这样容易欠拟合，欠拟合对应上图 High Bias，点偏离中心。低方差对应就是点都打的很集中，但不一定是靶心附近，手很稳，但不一定瞄的准。

我们常说集成学习中的基模型是弱模型，通常来说弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型，==**但并不是所有集成学习框架中的基模型都是弱模型**。**Bagging 和 Stacking 中的基模型为强模型（偏差低，方差高），而Boosting 中的基模型为弱模型（偏差高，方差低）**。==

### Bagging 的偏差与方差

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012258144.png" alt="image-20230901225808089" style="zoom: 50%;" />

通过上式我们可以看到：

- **整体模型的期望等于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似。**
- **整体模型的方差小于等于基模型的方差，当且仅当相关性为 1 时取等号，随着基模型数量增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。**但是，模型的准确度一定会无限逼近于 1 吗？并不一定，当基模型数增加到一定程度时，方差公式第一项的改变对整体方差的作用很小，防止过拟合的能力达到极限，这便是准确度的极限了。

在此我们知道了为什么 Bagging 中的基模型一定要为强模型，如果 Bagging 使用弱模型则会导致整体模型的偏差提高，而准确度降低。

Random Forest 是经典的基于 Bagging 框架的模型，并在此基础上通过引入特征采样和样本采样来降低基模型间的相关性，在公式中显著降低方差公式中的第二项，略微升高第一项，从而使得整体降低模型整体方差。

### Boosting 的偏差与方差

对于 Boosting 来说，由于基模型共用同一套训练集，所以基模型间具有强相关性，故模型间的相关系数近似等于 1，针对 Boosting 化简公式为：

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012300427.png" alt="image-20230901230016375" style="zoom:50%;" />

通过观察整体方差的表达式我们容易发现：

- 整体模型的方差等于基模型的方差，如果基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。因此，Boosting 框架中的基模型必须为弱模型。
- 此外 Boosting 框架中采用基于贪心策略的前向加法，整体模型的期望由基模型的期望累加而成，所以随着基模型数的增多，整体模型的期望值增加，整体模型的准确度提高。

基于 Boosting 框架的 Gradient Boosting Decision Tree 模型中基模型也为树模型，同 Random Forrest，我们也可以对特征进行随机抽样来使基模型间的相关性降低，从而达到减少方差的效果。



### 小结

- 我们可以使用模型的偏差和方差来近似描述模型的准确度；
- ==对于 Bagging 来说，整体模型的偏差与基模型近似，而随着模型的增加可以降低整体模型的方差，故其基模型需要为强模型；==
- ==对于 Boosting 来说，整体模型的方差近似等于基模型的方差，而整体模型的偏差由基模型累加而成，故基模型需要为弱模型。==

我们常说集成学习中的基模型是弱模型，通常来说弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型，==**但并不是所有集成学习框架中的基模型都是弱模型**。**Bagging 和 Stacking 中的基模型为强模型（偏差低，方差高），而Boosting 中的基模型为弱模型（偏差高，方差低）**。==

## bagging集成

机器学习算法中有两类典型的集成思想：bagging和bossting。

bagging是一种在原始数据集上，通过**有放回抽样**分别选出k个新数据集，来训练分类器的集成算法。分类器之间没有依赖关系。

随机森林属于bagging集成算法。**通过组合多个弱分类器，集思广益，使得整体模型具有较高的精确度和泛化性能**。



## 随机森林

Random Forest（随机森林），用随机的方式建立一个森林。RF 算法由很多决策树组成，每一棵决策树之间没有关联。建立完森林后，当有新样本进入时，每棵决策树都会分别进行判断，然后基于投票法给出分类结果。

我们将**使用CART决策树作为弱学习器的bagging方法称为随机森林**。

![img](https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012153869.webp)



由于随机性，**随机森林对于降低模型方差效果显著**。故随机森林一般不需要额外剪枝，就能取得较好的泛化性能。

相对而言，模型对于训练集的拟合程度就会差一些，相比于基于boosting的GBDT模型，偏差会大一些。

==另外，**随机森林中的树一般会比较深，以尽可能地降低偏差；而GBDT树的深度会比较浅，通过减少模型复杂度来降低方差**。*（面试考点）*==

最后，我们总结一下随机森林都有哪些优点：

- 采用了集成算法，精度优于大多数单模型算法
- 在测试集上表现良好，两个随机性的引入降低了过拟合风险
- 树的组合可以让随机森林处理非线性数据
- 训练过程中能检测特征重要性，是常见的**特征筛选**方法
- 每棵树可以同时生成，并行效率高，训练速度快
- 可以自动处理缺省值



### 思想

Random Forest（随机森林）是 Bagging 的扩展变体，它在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机特征选择，因此可以概括 RF 包括四个部分：

1. 随机选择样本（放回抽样）；
2. 随机选择特征；
3. 构建决策树；
4. 随机森林投票（平均）。

随机选择样本和 Bagging 相同，采用的是 Bootstrap 自助采样法；**随机选择特征是指在每个节点在分裂过程中都是随机选择特征的**（区别与每棵树随机选择一批特征）。

这种随机性导致随机森林的偏差会有稍微的增加（相比于单棵不随机树），但是由于随机森林的“平均”特性，会使得它的方差减小，而且方差的减小补偿了偏差的增大，因此总体而言是更好的模型。

随机采样由于引入了两种采样方法保证了随机性，所以每棵树都是最大可能的进行生长就算不剪枝也不会出现过拟合。

### 优缺点

**优点**

1. 在数据集上表现良好，相对于其他算法有较大的优势
2. 易于并行化，在大数据集上有很大的优势；
3. 能够处理高维度数据，不用做特征选择。



## Adaboost

AdaBoost（Adaptive Boosting，自适应增强），其自适应在于：**前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。**

### 思想

Adaboost 迭代算法有三步：

1. 初始化训练样本的权值分布，每个样本具有相同权重；
2. 训练弱分类器，如果样本分类正确，则在构造下一个训练集中，它的权值就会被降低；反之提高。用更新过的样本集去训练下一个分类器；
3. 将所有弱分类组合成强分类器，各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，降低分类误差率大的弱分类器的权重。

### 细节

#### 损失函数

Adaboost 模型是加法模型，学习算法为前向分步学习算法，损失函数为指数函数的分类问题。

**加法模型**：最终的强分类器是由若干个弱分类器加权平均得到的。

**前向分布学习算法**：算法是通过一轮轮的弱学习器学习，利用前一个弱学习器的结果来更新后一个弱学习器的训练集权重。

省略内容：

[【机器学习】决策树（中）——Random Forest、Adaboost、GBDT （非常详细） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/86263786)





#### **正则化**

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012313474.png" alt="image-20230901231301416" style="zoom:80%;" />

### 优缺点

**优点**

1. 分类精度高；
2. 可以用各种回归分类模型来构建弱学习器，非常灵活；
3. 不容易发生过拟合。

**缺点**

1. 对异常点敏感，异常点会获得较高权重。



## GBDT

GBDT（Gradient Boosting Decision Tree）是一种迭代的决策树算法，该算法由多棵决策树组成，从名字中我们可以看出来它是属于 Boosting 策略。GBDT 是被公认的泛化能力较强的算法。

### 思想

GBDT 由三个概念组成：Regression Decision Tree（即 DT）、Gradient Boosting（即 GB），和 Shrinkage（一个重要演变）

#### **回归树（Regression Decision Tree）**

如果认为 GBDT 由很多分类树那就大错特错了（虽然调整后也可以分类）。对于分类树而言，其值加减无意义（如性别），而对于回归树而言，其值加减才是有意义的（如说年龄）。GBDT 的核心在于累加所有树的结果作为最终结果，所以 GBDT 中的树都是回归树，不是分类树，这一点相当重要。

回归树在分枝时会穷举每一个特征的每个阈值以找到最好的分割点，衡量标准是最小化均方误差。

#### 梯度迭代（Gradient Boosting）

上面说到 GBDT 的核心在于累加所有树的结果作为最终结果，GBDT 的每一棵树都是以之前树得到的残差来更新目标值，这样每一棵树的值加起来即为 GBDT 的预测值。

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012317686.png" alt="image-20230901231752639" style="zoom:80%;" />

![image-20230901231859279](https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012318328.png)

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012319528.png" alt="image-20230901231910479" style="zoom:50%;" />



<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012319109.png" alt="image-20230901231930054" style="zoom:80%;" />

==那么 Gradient 从何体现？其实很简单，其**残差其实是最小均方损失函数关于预测值的反向梯度(划重点)**：==

<img src="https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012320540.png" alt="image-20230901232017490" style="zoom:50%;" />

也就是说，预测值和实际值的残差与损失函数的负梯度相同。

但要注意，基于残差 GBDT 容易对异常值敏感，举例：

![img](https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012320524.webp)

GBDT 的 Boosting 不同于 Adaboost 的 Boosting，**GBDT 的每一步残差计算其实变相地增大了被分错样本的权重，而对与分对样本的权重趋于 0**，这样后面的树就能专注于那些被分错的样本。

[【机器学习】决策树（中）——Random Forest、Adaboost、GBDT （非常详细） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/86263786)



#### **缩减（Shrinkage）**

Shrinkage 的思想认为，每走一小步逐渐逼近结果的效果要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它并不是完全信任每一棵残差树。

![image-20230901232136726](https://zhaozhao626.oss-cn-shenzhen.aliyuncs.com/zhaozhao/202309012321767.png)

Shrinkage 不直接用残差修复误差，而是只修复一点点，把大步切成小步。本质上 Shrinkage 为每棵树设置了一个 weight，累加时要乘以这个 weight，当 weight 降低时，基模型数会配合增大。



### 优缺点

**优点**

1. 可以自动进行特征组合，拟合非线性数据；
2. 可以灵活处理各种类型的数据。

**缺点**

1. 对异常点敏感。

### 与 Adaboost 的对比

**5.3.1 相同：**

1. 都是 Boosting 家族成员，使用弱分类器；
2. 都使用前向分布算法；

**5.3.2 不同：**

1. **迭代思路不同**：Adaboost 是通过提升错分数据点的权重来弥补模型的不足（利用错分样本），而 GBDT 是通过算梯度来弥补模型的不足（利用残差）；
2. **损失函数不同**：AdaBoost 采用的是指数损失，GBDT 使用的是绝对损失或者 Huber 损失函数；



## XGBoost

[【机器学习】决策树（下）——XGBoost、LightGBM（非常详细） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/87885678)



### 优缺点

**1.3.1 优点**

1. **精度更高：**GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数；
2. **灵活性更强：**GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，（使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题））。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导；
3. **正则化：**XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合；
4. **Shrinkage（缩减）：**相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间；
5. **列抽样：**XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算；
6. **缺失值处理：**XGBoost 采用的稀疏感知算法极大的加快了节点分裂的速度；
7. **可以并行化操作：**块结构可以很好的支持并行计算。

**1.3.2 缺点**

1. 虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；
2. 预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。



## LightGBM





### 与 XGBoost 的对比

本节主要总结下 LightGBM 相对于 XGBoost 的优点，从内存和速度两方面进行介绍。

**2.3.1 内存更小**

1. XGBoost 使用预排序后需要记录特征值及其对应样本的统计值的索引，而 LightGBM 使用了直方图算法将特征值转变为 bin 值，且不需要记录特征到样本的索引，将空间复杂度从 �(2∗#����) 降低为 �(#���) ，极大的减少了内存消耗；
2. LightGBM 采用了直方图算法将存储特征值转变为存储 bin 值，降低了内存消耗；
3. LightGBM 在训练过程中采用互斥特征捆绑算法减少了特征数量，降低了内存消耗。

**2.3.2 速度更快**

1. LightGBM 采用了直方图算法将遍历样本转变为遍历直方图，极大的降低了时间复杂度；
2. LightGBM 在训练过程中采用单边梯度算法过滤掉梯度小的样本，减少了大量的计算；
3. LightGBM 采用了基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量；
4. LightGBM 采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略；
5. LightGBM 对缓存也进行了优化，增加了 Cache hit 的命中率。

## 参考

[认真的聊一聊决策树和随机森林 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/347594334)

[【机器学习】决策树（上）——ID3、C4.5、CART（非常详细） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/85731206)



